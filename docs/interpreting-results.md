# Interpreting Benchmark Results

This document explains how to interpret the results generated by the benchmarking library. Understanding these results will help you assess the performance and reliability of your code.

## Understanding the Benchmark Report

The benchmark library produces a `BenchmarkReport` for each benchmarked function. This report contains various statistical metrics that provide insights into the function's performance.

Here's a breakdown of each property in the `BenchmarkReport` and how to interpret it:

```typescript
interface BenchmarkReport {
  kind: 'benchmark';
  name: string;
  operationsPerSecond: number;
  relativeMarginOfError: number;
  sampleStandardDeviation: number;
  sampleArithmeticMean: number;
  marginOfError: number;
  executionTimes: number[];
  standardErrorOfTheMean: number;
  sampleVariance: number;
  samples: number;
  date: string;
}
```

## Key Metrics and Their Interpretation

### **name**: `string`

- **What it is**: The name of the benchmarked function.
- **How to use it**: This helps you identify which function the results belong to.
- **Example**: If you benchmarked a function called `myFastFunction`, the name would be `myFastFunction`.

### **operationsPerSecond**: `number`

- **What it is**: The number of times the benchmarked function can be executed per second.
- **How to use it**: This is a primary indicator of performance. Higher values are better. It tells you how many times your function can run in one second.
- **Example**:
  - `1,000,000` ops/sec (1 million operations per second) is excellent.
  - `100` ops/sec is very slow.
- **Guideline** If you are comparing two functions, the one with the higher `operationsPerSecond` is faster.

### **sampleArithmeticMean**: `number`

- **What it is**: The average execution time of the benchmarked function, measured in seconds.
- **How to use it**: This is another primary indicator of performance. Lower values are better. It tells you how long, on average, it takes for your function to run once.
- **Example**:
  - `0.000001` seconds (1 microsecond) is extremely fast.
  - `0.1` seconds (100 milliseconds) is relatively slow.
- **Guideline** If you are comparing two functions, the one with the lower `sampleArithmeticMean` is faster.

### **relativeMarginOfError (RME)**: `number`

- **What it is**: A measure of the precision of the benchmark results. It indicates how much the results might vary if you were to run the benchmark again.
- **How to use it**: Lower values are better. A lower RME means the results are more reliable and consistent.
- **Guidelines**:
  - **RME <= 0.05 (5%)**: Generally considered good precision. The results are relatively reliable.
  - **0.05 < RME <= 0.1 (5% to 10%)**: Moderate precision. The results might have some variability. Consider running the benchmark again or increasing the number of samples.
  - **RME > 0.1 (10%)**: Low precision. The results are less reliable. Investigate the cause of the high variability (see "Troubleshooting High RME" below).
- **Example**:
  - `0.01` (1% RME) is excellent.
  - `0.07` (7% RME) is acceptable but could be better.
  - `0.2` (20% RME) is concerning and indicates a problem.

### sampleStandardDeviation: `number`

- **What it is**: A measure of the consistency of the execution times. It tells you how much the individual execution times varied from the average.
- **How to use it**: Lower values are better. A lower standard deviation means the function's execution time is more consistent.
- **Guidelines**:
  - **Standard Deviation <= 0.01 (1 millisecond)**: Very consistent.
  - **0.01 < Standard Deviation <= 0.05 (1 to 5 milliseconds)**: Moderately consistent.
  - **Standard Deviation > 0.05 (5 milliseconds)**: Inconsistent. Investigate the cause of the high variability.
- **Example**:
  - `0.0005` (0.5 milliseconds) is very consistent.
  - `0.02` (2 milliseconds) is moderately consistent.
  - `0.1` (10 milliseconds) is inconsistent.

### marginOfError: `number`

- **What it is**: A measure of the uncertainty in the results. It's calculated based on the standard deviation and the number of samples.
- **How to use it**: Lower values are better. A lower margin of error means you can be more confident in the results.
- **Guidelines**:
  - **Margin of Error <= 0.01 (1 millisecond)**: Very reliable.
  - **0.01 < Margin of Error <= 0.05 (1 to 5 milliseconds)**: Moderately reliable.
  - **Margin of Error > 0.05 (5 milliseconds)**: Less reliable. Investigate the cause of the high uncertainty.
- **Example**:
  - `0.0001` (0.1 milliseconds) is very reliable.
  - `0.03` (3 milliseconds) is moderately reliable.
  - `0.1` (10 milliseconds) is less reliable.

### standardErrorOfTheMean: `number`

- **What it is**: A measure of how precise the `sampleArithmeticMean` is. It estimates how much the sample mean might differ from the true mean.
- **How to use it**: Lower values are better. A lower standard error of the mean means the sample mean is a more accurate estimate of the true mean.
- **Example**:
  - `0.00001` is very precise.
  - `0.001` is less precise.

## sampleVariance: `number`

- **What it is**: Another measure of the spread or dispersion of the execution times. It's the square of the standard deviation.
- **How to use it**: Lower values are better. A lower variance means the execution times are more tightly clustered around the mean.
- **Example**:
  - `0.0000001` is very consistent.
  - `0.001` is less consistent.

### samples: `number`

- **What it is**: The number of times the benchmarked function was executed during the measurement phase.
- **How to use it**: Higher values are generally better. More samples lead to more reliable results.
- **Guidelines**:
  - **Samples < 10**: The results might be unreliable.
  - **Samples >= 10**: Generally acceptable, but more is better.
  - **Samples >= 100**: Very reliable.
- **Example**:
  - `5` samples are not enough.
  - `50` samples are acceptable.
  - `500` samples are very good.

### executionTimes: `number[]`

- **What it is**: An array of the individual execution times (in milliseconds) for each iteration of the benchmarked function.
- **How to use it**: This array is useful for in-depth analysis. You can:
  - Visualize the distribution of execution times (e.g., using a histogram).
  - Identify outliers.
  - Calculate other statistical measures.
- **Guideline** The array is ordered. The first element is the fastest execution time, and the last element is the slowest.
- **Example**: [0.09, 0.095, 0.1, 0.105, 0.11, 0.12]

### date: `string`

- **What it is**: The date when the benchmark was run.
- **How to use it**: This is useful for tracking changes in performance over time.

## Troubleshooting High RME

If you encounter a high Relative Margin of Error (RME), here are some steps to investigate:

- **Check `sampleStandardDeviation`**: A high standard deviation indicates inconsistent execution times.
- **Check `executionTimes`**: Look for outliers (extremely slow or fast runs).
- **Increase `maxIterations`**: More samples can help reduce the impact of variability.
- **Increase `maxExecutionTime`**: If the benchmark is stopping too early, increase this value.
- **Isolate the Test Environment**: Close unnecessary applications and minimize background processes.
- **Adjust `timeThreshold`**: If your function is very fast, try increasing `timeThreshold` to get more accurate measurements.
- **Check the benchmark function** If the benchmark function performs asynchronous operations, the timing of these operations can vary, leading to inconsistent results. Try to reduce the number of asynchronous operations in the benchmark function.

## Timer Resolution Limitations

When benchmarking extremely fast functions, you might encounter a situation where the `sampleStandardDeviation`, `marginOfError`, and `sampleArithmeticMean` are all reported as zero, while the `relativeMarginOfError` (RME) is high. This is often a sign that the `performance.now()` timer is not precise enough to measure the function's execution time accurately.

### What Happens

- **Rounding to Zero**: If the function executes in a fraction of a millisecond, the timer might round the execution time down to zero for every measurement.
- **Zero Standard Deviation**: If all measurements are zero, the standard deviation (which measures the spread of the data) will also be zero.
- **Zero Margin of Error**: The margin of error is calculated based on the standard deviation, so it will also be zero.
- **Zero Arithmetic Mean** The arithmetic mean is calculated based on the execution times, so it will also be zero.
- **High RME**: The RME is calculated by dividing the margin of error by the mean. If the mean is zero (or very close to zero), the RME will be undefined or very high.

### How to Recognize This Issue

- **Zero `sampleStandardDeviation`**: This is the most important indicator.
- **Zero `marginOfError`**: This is another strong indicator.
- **Zero `sampleArithmeticMean`** This is another strong indicator.
- **High `relativeMarginOfError`**: A high RME, especially when combined with the above, is a sign of this issue.
- Warnings in the logs The logs will contain warnings about the zero values.

### How to Address It

- **Increase `timeThreshold`**: Try increasing the timeThreshold option. This will force the benchmark to run the function more times within each measurement, which might make the measurements more accurate.
- **Increase `innerIterations`**: Try increasing the innerIterations option. This will also force the benchmark to run the function more times within each measurement.
- **Check the benchmark function** Make sure that the benchmark function is correct and that it is not doing anything that might interfere with the measurements.
- **Check the code** Make sure that the code is correct and that there are no errors.
- **Check the environment** Make sure that the environment is stable and that there are no other processes that might be interfering with the measurements.

## Comparing Benchmarks

When comparing the performance of two functions (e.g., functionA vs. functionB), focus on these metrics:

- **operationsPerSecond**: The function with the higher `operationsPerSecond` is generally faster.
- **sampleArithmeticMean**: The function with the lower `sampleArithmeticMean` is generally faster.
- **relativeMarginOfError**: Make sure both functions have a low RME to ensure the comparison is reliable.
- **sampleStandardDeviation** Make sure both functions have a low `sampleStandardDeviation` to ensure the comparison is reliable.
- **marginOfError** Make sure both functions have a low `marginOfError` to ensure the comparison is reliable.

## Conclusion

By carefully examining the metrics in the `BenchmarkReport`, you can gain valuable insights into the performance and reliability of your code. Remember that benchmarking is an iterative process, and it often takes some experimentation to get the most accurate and meaningful results.
